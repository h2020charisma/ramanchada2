{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "- check if .hscfg file in your home folder is correctly configured (see hsdsaccess_example.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = \"/SILICON_STUDY/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ramanchada2.spectrum import from_chada\n",
    "from ramanchada2.io.HSDS import visit_domain, filter_dataset, read_cha\n",
    "import math\n",
    "import h5pyd\n",
    "\n",
    "\n",
    "def load_dataset(parentdomain,domain,results=[],h5module=h5pyd):\n",
    "    with h5module.File(domain) as f:\n",
    "        wavelength = f[\"annotation_study\"].attrs[\"wavelength\"]\n",
    "        instrument = f[\"annotation_study\"].attrs[\"instrument\"]\n",
    "        laser_power = f[\"annotation_study\"].attrs[\"laser_power\"]\n",
    "        provider = f[\"annotation_study\"].attrs[\"provider\"]\n",
    "        sample = f[\"annotation_sample\"].attrs[\"sample\"]\n",
    "    results.append((domain,provider,instrument,wavelength,laser_power,sample))\n",
    "\n",
    "query_sample = \"S0N\"\n",
    "\n",
    "results = []\n",
    "# query by sample\n",
    "#visit_domain(study, process_dataset=filter_dataset,kwargs={\"process_file\" : load_dataset,\"sample\": query_sample, \n",
    "#                            \"kwargs\" : {\"results\" : results}})\n",
    "# retrieve everything under study\n",
    "visit_domain(study, process_dataset=load_dataset, kwargs = {\"results\" : results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results,columns=[\"domain\",\"provider\",\"instrument\",\"wavelength\",\"laser_power\",\"sample\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row(row):\n",
    "    print(row[\"domain\"])\n",
    "    #apply peak finding tbd\n",
    "    \n",
    "df.apply(lambda row: process_row(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "#test follows, to be applied to all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = df[df[\"sample\"]==\"S0B\"].iloc[0][\"domain\"]\n",
    "spe = from_chada(domain,h5module=h5pyd)\n",
    "           # method: Literal['x-axis', 'bins'],\n",
    "#spe.plot()\n",
    "spe = spe.trim_axes(\"x-axis\",[450,600])\n",
    "#spe = spe.normalize(\"unity\")\n",
    "spe.plot()\n",
    "bgm = spe.bayesian_gaussian_mixture(n_samples=10000,  # type: ignore\n",
    "            n_components=5,\n",
    "            max_iter=1000,\n",
    "            moving_minimum_window=16,\n",
    "            random_state=42,\n",
    "            trim_range=None)\n",
    "#spe = spe.normalize(\"unity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_area(spe):\n",
    "    res = spe.y # - np.min(spe.y)\n",
    "    res /= (np.sum(res) * (spe.x[1]-spe.x[0]))\n",
    "    return res\n",
    "\n",
    "def plotdist(bgm_peaks,spe,threshold=0.00001):\n",
    "    new_spe = spe.normalize('unity')\n",
    "    spe = spe - spe.moving_minimum(16)    \n",
    "    y_norm = normalize_area(spe)\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,2))\n",
    "    ax1.plot(spe.x,y_norm,':',label='spe')\n",
    "    n_clusters = (np.round(bgm.weights_, 2) > 0).sum()\n",
    "    _tmp = None\n",
    "    for e in bgm_peaks:\n",
    "        weight = e[2]\n",
    "        if weight>=threshold:\n",
    "            mu = e[0]\n",
    "            sigma =e[1]\n",
    "            print(sigma)\n",
    "            gm = stats.norm(mu, sigma)\n",
    "            if _tmp is None:\n",
    "                _tmp = weight*gm.pdf(spe.x)\n",
    "            else:\n",
    "                _tmp = _tmp + weight*gm.pdf(spe.x)\n",
    "            #print(_tmp)\n",
    "            ax1.scatter(mu, weight*gm.pdf(mu),label=e[1])\n",
    "            ax2.plot(spe.x, weight*gm.pdf(spe.x),'+')\n",
    "            ax2.plot(spe.x,y_norm,':')\n",
    "            ax2.plot(spe.x,new_spe.y,'*')\n",
    "    ax1.plot(spe.x,_tmp,'-')\n",
    "    return (ax1,ax2)\n",
    "\n",
    "bgm_peaks = [[mean[0], np.sqrt(cov[0][0]), weight]\n",
    "                     for mean, cov, weight in\n",
    "                     zip(bgm.means_, bgm.covariances_, bgm.weights_)]\n",
    "bgm_peaks = sorted(bgm_peaks, key=lambda x: x[2], reverse=True)\n",
    "n_peaks = (np.round(bgm.weights_, 2) > 0).sum()\n",
    "bgm_peaks = bgm_peaks[:n_peaks]\n",
    "print(bgm_peaks)\n",
    "plotdist(bgm_peaks,spe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('charisma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb04d3982f35e0acdef77e55e8b663fd61d1c7b237bd8280d18cbffb390c1e7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
